= Report

== Abbreviations used


== Introduction

=== Input
We use each pixel(normalized) of 2d Gray scale image as an input to our Neural Network.
Since, the images from MNIST database are of 28x28 pixels, we have 784 inputs.

=== Output
The outputs labels from MNIST database are converted to one-hot encoding.
From out network, for each class we receive the probability values which we further process it to get
various metrics such as accuracy, ROC, Precision-Recall, Confusion Matrix etc.
To avoid, unnecessary blotting of the report, we have the plots whenever there's something interesting.

=== Training Procedure
Out of 70,000 images from MNSIT database, we use 60,000 images as training samples.

=== Testing Procedure
Out of 70,000 images from MNSIT database, we use 10,000 images as testing samples.

=== Evaluation Procedure
Since we have multi-class classification problem, we use *categorical_accuracy*.
It checks to see if the index of the maximal true value is equal to the index of the maximal predicted value.

[NOTE]
====
We can use this as loss function as well. We experiment with various loss functions, but we use this as an
overall metric to compare models and configurations.
====

== Baseline
We start our study with a very simple network having just one hidden layer

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense10softmax/list.adoc[tags=model]

=== Number of neurons in hidden layer
As starting point, the common literature on online suggests:
----
Number of Outputs <= #neurons <= Number of inputs
#neurons ~ Sum(Number of inputs, Number of Outputs) x 2/3
----
In our case, Sum(Number of inputs, Number of Outputs) x 2/3 = 529.33 ~ 530
Moreover, for reasons of speed, it is advised to use powers of 2.
Hence, an ideal value would be: 512. However, to be sure we test with both 256 and 512 neurons

While we evaluate number of neurons, we keep the following parameters constant:
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense10softmax/list.adoc[tags=config]

==== Results
* 256 Neurons
+
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense256reluDense10softmax/list.adoc[tags=result]
* 512 Neurons
+
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense10softmax/list.adoc[tags=result]

We see a big increase in accuracy by choosing 512 neurons instead of 256 and a reduction in loss.
Having more number of neurons means more features can be learnt.
To test that hypothesis, when we fed the test data in same order and picked the 1st 9 errors under both the configuration:

* 256 Neurons:
+
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense256reluDense10softmax/list.adoc[tags=roc_curve]

* 512 Neurons:
+
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense10softmax/list.adoc[tags=roc_curve]

As we see one of the ROC curves of 256 neuron network goes below the diagonal, from image we can infer that model
is not able to confidently decide 'not 9'.
hence it's better to have 512 neurons than 256.


With 128 Neurons:

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense128reluDense10softmax/list.adoc[tags=incorrect]
We clearly observe that with 128, we find more errors of same type.
Therefore, by __having more number of neurons more features can be learnt__.
Hence, for *upcoming experiments we use 512 neurons*.

=== Number of hidden layers
One of the thumb rules used in industry to judge number of hidden layers:
----
0: linearly separable
1: continuous functions
2: arbitrary decision boundary
> 2: complex representations
----
Our data is neither linearly separable nor continuous, hence 1 layer as in last section may not be a good choice.
Hence, in this section we try 2, 3 and 4 layers.

For all the tests we keep the following configuration:
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=config]

==== Results
* 2 hidden Layers
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]
* 3 hidden Layers
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=result]
* 4 hidden Layers
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=result]

If we examine the metrics plot:

* 2 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
* 3 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
* 4 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

The validation accuracy is higher than the training accuracy and also the validation loss is lower than
the training loss. This suggests under-fitting. which we will need to eliminate.
Since the gap between the training and validation plots becomes more pronounced for higher number of hidden layers,
we can say that when we have more layers, the model learns 'fast'(premature) and hence under-fits the data.

Maybe if we train the model for more longer time, will more layers provide better results?:
We try to check the impact by increasing the number of epochs to 8:

* 2 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e8b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]
* 3 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e8b8192mDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=result]
* 4 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e8b8192mDense512reluDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=result]

Even if we increase the number of epochs, 2 layers perform better.
Also, 2 layers is computationally less heavy as well.
So, __having more layers will not necessarily improve accuracy.__
Therefore, *we will be using 2 hidden layers for all the upcoming tests*

=== Number of epochs
In the previous section we saw results for 2 hidden layers:

* 4 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

* 8 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e8b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

The model and config that we carry from previous section:

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=config]

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=model]

Now under the same configuration, we experiment with 8, 16, 32, 64 .... until, if possible, we are able to solve
under-fitting.

==== Results
The plots below show the metrics of network trained under:

* 16 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e16b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* 64 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* 256 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e256b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

We can see that though we seem to achieve higher accuracy by training the network for more epochs, we dont seem to have
solved the problem of under-fitting.

Moreover, when considering the trend of increase in accuracy, we have few interesting observations.

Below is the data of 3 of the models trained under various epochs:

* 64 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

* 128 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e128b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

* 256 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e256b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

The plot below displays accuracy and loss on testing data by networks which are trained for:
1, 2, 4, 8, 16, 32, 64, 128 and 256 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e256b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=special]

As we can see from the graph, though we train the model each time with double the number of epochs as before,
we don't necessarily obtain a big increase as we go for higher values(though we still continue to suffer from
under-fitting). It hits a plateau earlier.

Considering the above trend, maybe to reach accuracy > 95%, we'll need to train model for 1024 or maybe 2056 epochs.

Hence, just increasing the number of epochs may not be a good idea. In other words, __beyond a certain point, the
increase in accuracy will be much lesser in relation to increase in number of epochs__.

Moreover, to judge the number of epochs for further experimentation, if we see the ROC curves for

* 16 epochs
+
include::./op/sgdcategorical_crossentropyrelul0.01e16b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

include::./op/sgdcategorical_crossentropyrelul0.01e32b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

* 32 epochs
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

include::./op/sgdcategorical_crossentropyrelul0.01e32b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

* 64 epochs
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

We see that anything beyond 16 epochs the model is more 'confident', since it starts showing better area under roc
for most classes. However, from precision recall curve we can see that when the model is put to test with all the
samples though it's more confident it's predictions are not necessarily perfect.

In order to confirm above hypothesis, we provide below few examples:
For 64 epochs:

* Correctly classified images
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=correct]

* Incorrect
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=incorrect]

For example, for class 1, our area under roc is 1.0 but if we see that images, just tilting one to the other
side can cause our model to not recognize 1 anymore. Hence, we need to improve the precision-recall curve.

Since, both ROC and precision-recall of 64 epochs is better, we choose to evaluate further with 64 and 32(just in case)
by varying the batch size. As, that introduces more updates to model, we expect the model to learn more features and
hence an improvement in precision-recall curve.

=== Batch size
We started our experiments with an arbitrary huge batch size = 8192.
In this section, we use the same configuration as before except for the batch sizes:
include::./op/sgdcategorical_crossentropyrelul0.01e16b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=config]

We performed experiments with various batch sizes of 4096, 2048, 1024, 512, 256, 128, 64, 32. Below we provide results for select few.

* At 32 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b32mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
** 128 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b128mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
** 128 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b128mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

As we can see from the metrics plots, somewhere about a batch size of 128, we start to see accuracy of training going
higher than validation. This could mean that finally we can solve the problem of under-fitting by choosing
batch sizes < 128 for 32 and 64 epochs.

Also for smaller batches of 32 and 64(almost), we observe that it reaches a constant value in validation accuracy.
Hence, this would be the best possible accuracy that this model could have and we can infact stop training the model
when it reaches a constant value.
For a batch size of 64, we can stop somewhere around 30 epoch and for batch size of 32,
we can stop somewhere around 20 epochs.

To confirm, if there's a really big of a difference between the 2, we see the precision recall curve:

* At 32 epochs:
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

If we check roc curves:

* At 32 epochs:
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

Hence, both the models are able to distinguish not just with full confidence but actually correct in most instances.
This is evident by the result:

* At 32 epochs:
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

To confirm, if we examine incorrectly classified outputs:

* At 32 epochs:
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=incorrect]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=incorrect]

We see that when the shape is slightly distorted or if the thickness is too big, the model fails to predict correctly.
Also, to be fair, instances like 2 and 8 in the picture can confuse even a person on first glance.

Important thing to notice is that, both the models classify incorrectly almost the same images. One is better than
the other by just 0.6% i,e, one model classifies 20-25 images more correctly than the other for given set of
10000 validation images.

Since 32 epochs with 64 batch size is significantly faster than the vice-versa, we pick this configuration for further study.

