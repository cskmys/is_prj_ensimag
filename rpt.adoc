= Report

== Introduction




== Conventional Neural Network
=== Parameters
* Input:
+
We use each pixel(normalized) of 2d Gray scale image as an input to our Neural Network.
Since, the images from MNIST database are of 28x28 pixels, we have 784 inputs.

* Output:
+
The outputs labels from MNIST database are converted to one-hot encoding.
From out network, for each class we receive the probability values which we further process it to get
various metrics such as accuracy, ROC, Precision-Recall, Confusion Matrix etc.
To avoid, unnecessary blotting of the report, we have the plots whenever there's something interesting.

=== Procedure
* Training
+
Out of 70,000 images from MNSIT database, we use 60,000 images as training samples.

* Testing
+
Out of 70,000 images from MNSIT database, we use 10,000 images as testing samples.

* Evaluation
+
Since we have multi-class classification problem, we use *categorical_accuracy*.
It checks to see if the index of the maximal true value is equal to the index of the maximal predicted value.

=== Baseline
We start our study with a very simple network having just one hidden layer

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense10softmax/list.adoc[tags=model]

=== Number of neurons in hidden layer
As starting point, the common literature on online suggests:
----
Number of Outputs <= #neurons <= Number of inputs
#neurons ~ Sum(Number of inputs, Number of Outputs) x 2/3
----
In our case, Sum(Number of inputs, Number of Outputs) x 2/3 = 529.33 ~ 530
Moreover, for reasons of speed, it is advised to use powers of 2.
Hence, an ideal value would be: 512. However, to be sure we test with both 256 and 512 neurons

While we evaluate number of neurons, we keep the following parameters constant:
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense10softmax/list.adoc[tags=config]

==== Results
* 256 Neurons
+
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense256reluDense10softmax/list.adoc[tags=result]
* 512 Neurons
+
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense10softmax/list.adoc[tags=result]

We see a big increase in accuracy by choosing 512 neurons instead of 256 and a reduction in loss.
Having more number of neurons means more features can be learnt.
To test that hypothesis, when we fed the test data in same order and picked the 1st 9 errors under both the configuration:

* 256 Neurons:
+
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense256reluDense10softmax/list.adoc[tags=roc_curve]

* 512 Neurons:
+
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense10softmax/list.adoc[tags=roc_curve]

As we see one of the ROC curves of 256 neuron network goes below the diagonal, from image we can infer that model
is not able to confidently decide 'not 9'.
hence it's better to have 512 neurons than 256.


With 128 Neurons:

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense128reluDense10softmax/list.adoc[tags=incorrect]
We clearly observe that with 128, we find more errors of same type.
Therefore, by __having more number of neurons more features can be learnt__.
Hence, for *upcoming experiments we use 512 neurons*.

=== Number of hidden layers
One of the thumb rules used in industry to judge number of hidden layers:
----
0: linearly separable
1: continuous functions
2: arbitrary decision boundary
> 2: complex representations
----
Our data is neither linearly separable nor continuous, hence 1 layer as in last section may not be a good choice.
Hence, in this section we try 2, 3 and 4 layers.

For all the tests we keep the following configuration:
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=config]

==== Results
* 2 hidden Layers
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]
* 3 hidden Layers
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=result]
* 4 hidden Layers
include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=result]

If we examine the metrics plot:

* 2 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
* 3 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
* 4 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

The validation accuracy is higher than the training accuracy and also the validation loss is lower than
the training loss. This suggests under-fitting. which we will need to eliminate.
Since the gap between the training and validation plots becomes more pronounced for higher number of hidden layers,
we can say that when we have more layers, the model learns 'fast'(premature) and hence under-fits the data.

Maybe if we train the model for more longer time, will more layers provide better results?:
We try to check the impact by increasing the number of epochs to 8:

* 2 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e8b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]
* 3 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e8b8192mDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=result]
* 4 hidden Layers

include::./op/sgdcategorical_crossentropyrelul0.01e8b8192mDense512reluDense512reluDense512reluDense512reluDense10softmax/list.adoc[tags=result]

Even if we increase the number of epochs, 2 layers perform better.
Also, 2 layers is computationally less heavy as well.
So, __having more layers will not necessarily improve accuracy.__
Therefore, *we will be using 2 hidden layers for all the upcoming tests*

=== Number of epochs
In the previous section we saw results for 2 hidden layers:

* 4 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

* 8 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e8b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

The model and config that we carry from previous section:

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=config]

include::./op/sgdcategorical_crossentropyrelul0.01e4b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=model]

Now under the same configuration, we experiment with 8, 16, 32, 64 .... until, if possible, we are able to solve
under-fitting.

==== Results
The plots below show the metrics of network trained under:

* 16 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e16b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* 64 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* 256 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e256b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

We can see that though we seem to achieve higher accuracy by training the network for more epochs, we dont seem to have
solved the problem of under-fitting.

Moreover, when considering the trend of increase in accuracy, we have few interesting observations.

Below is the data of 3 of the models trained under various epochs:

* 64 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

* 128 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e128b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

* 256 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e256b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

The plot below displays accuracy and loss on testing data by networks which are trained for:
1, 2, 4, 8, 16, 32, 64, 128 and 256 epochs

include::./op/sgdcategorical_crossentropyrelul0.01e256b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=special]

As we can see from the graph, though we train the model each time with double the number of epochs as before,
we don't necessarily obtain a big increase as we go for higher values(though we still continue to suffer from
under-fitting). It hits a plateau earlier.

Considering the above trend, maybe to reach accuracy > 95%, we'll need to train model for 1024 or maybe 2056 epochs.

Hence, just increasing the number of epochs may not be a good idea. In other words, __beyond a certain point, the
increase in accuracy will be much lesser in relation to increase in number of epochs__.

Moreover, to judge the number of epochs for further experimentation, if we see the ROC curves for

* 16 epochs
+
include::./op/sgdcategorical_crossentropyrelul0.01e16b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

include::./op/sgdcategorical_crossentropyrelul0.01e32b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

* 32 epochs
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

include::./op/sgdcategorical_crossentropyrelul0.01e32b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

* 64 epochs
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

We see that anything beyond 16 epochs the model is more 'confident', since it starts showing better area under roc
for most classes. However, from precision recall curve we can see that when the model is put to test with all the
samples though it's more confident it's predictions are not necessarily perfect.

In order to confirm above hypothesis, we provide below few examples:
For 64 epochs:

* Correctly classified images
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=correct]

* Incorrect
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=incorrect]

For example, for class 1, our area under roc is 1.0 but if we see that images, just tilting one to the other
side can cause our model to not recognize 1 anymore. Hence, we need to improve the precision-recall curve.

Since, both ROC and precision-recall of 64 epochs is better, we choose to evaluate further with 64 and 32(just in case)
by varying the batch size. As, that introduces more updates to model, we expect the model to learn more features and
hence an improvement in precision-recall curve.

=== Batch size
We started our experiments with an arbitrary huge batch size = 8192.
In this section, we use the same configuration as before except for the batch sizes:
include::./op/sgdcategorical_crossentropyrelul0.01e16b8192mDense512reluDense512reluDense10softmax/list.adoc[tags=config]

We performed experiments with various batch sizes of 4096, 2048, 1024, 512, 256, 128, 64, 32. Below we provide results for select few.

* At 32 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b32mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
** 128 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b128mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]
** 128 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b128mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

As we can see from the metrics plots, somewhere about a batch size of 128, we start to see accuracy of training going
higher than validation. This could mean that finally we can solve the problem of under-fitting by choosing
batch sizes < 128 for 32 and 64 epochs.

Also for smaller batches of 32 and 64(almost), we observe that it reaches a constant value in validation accuracy.
Hence, this would be the best possible accuracy that this model could have and we can infact stop training the model
when it reaches a constant value.
For a batch size of 64, we can stop somewhere around 30 epoch and for batch size of 32,
we can stop somewhere around 20 epochs.

To confirm, if there's a really big of a difference between the 2, we see the precision recall curve:

* At 32 epochs:
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

If we check roc curves:

* At 32 epochs:
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

Hence, both the models are able to distinguish not just with full confidence but actually correct in most instances.
This is evident by the result:

* At 32 epochs:
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=result]

To confirm, if we examine incorrectly classified outputs:

* At 32 epochs:
** 64 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=incorrect]

* At 64 epochs:
** 32 batch size
+
include::./op/sgdcategorical_crossentropyrelul0.01e64b32mDense512reluDense512reluDense10softmax/list.adoc[tags=incorrect]

We see that when the shape is slightly distorted or if the thickness is too big, the model fails to predict correctly.
Also, to be fair, instances like 2 and 8 in the picture can confuse even a person on first glance.

Important thing to notice is that, both the models classify incorrectly almost the same images. One is better than
the other by just 0.6% i,e, one model classifies 20-25 images more correctly than the other for given set of
10000 validation images.

Since 32 epochs with 64 batch size is significantly faster than the vice-versa, we pick this configuration for further study.

=== Activation function
For this section we will use the following configuration from the last section, while modifying the activation functions.
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=config]

[NOTE]
====
We don't change the activation of output layer since for a multi-class classification, softmax is the most apt amongst
the metrics available in Keras for our problem.
====

Since we use, sgd(stochastic gradient descent) as the algorithm for back-propagation, we have to choose a function
that is differentiable. Also, we need to choose a non-linear function to help network fit complex pattern.
Hence, we picked the following activation functions:

* Relu:
+
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=result]
+
[source, json]
----
{
    "training_time": 49.7351598739624
}
----

* Sigmoid:
+
include::./op/sgdcategorical_crossentropysigmoidl0.01e32b64mDense512sigmoidDense512sigmoidDense10softmax/list.adoc[tags=result]
+
[source, json]
----
{
    "training_time": 52.25036144256592
}
----

* Hard-Sigmoid:
+
include::./op/sgdcategorical_crossentropyhard_sigmoidl0.01e32b64mDense512hard_sigmoidDense512hard_sigmoidDense10softmax/list.adoc[tags=result]
+
[source, json]
----
{
    "training_time": 56.87085223197937
}
----

* tanh:
+
include::./op/sgdcategorical_crossentropytanhl0.01e32b64mDense512tanhDense512tanhDense10softmax/list.adoc[tags=result]
+
[source, json]
----
{
    "training_time": 51.18115472793579
}
----

* linear:
+
include::./op/sgdcategorical_crossentropylinearl0.01e32b64mDense512linearDense512linearDense10softmax/list.adoc[tags=result]
+
[source, json]
----
{
    "training_time": 52.688600301742554
}
----

As usual *Relu* provided highest accuracy with shorter training time of 49 seconds, while hard_sigmoid takes the
longest amount of time.

In terms of accuracy, we see that the non-linear functions that are centered around zero provide good accuracy while
those centered at 0.5 such as sigmoid and hard_sigmoid performed the worst.

Along with non-linear functions above, out of curiosity, we tested the linear function of keras which takes input
tensor as the activation value. We are bit surprised to see that it infact performs, giving an accuracy of 92%!
One reasoning we had from observing other function results is that, maybe the constant value obtained after
differentiation is centered around 0 and 0.5.
Hence, it performs better than functions centered at 0.5 and worse than those at 0.

=== Optimizer and Learning rates
We carry the same configuration from previous section but vary the optimizers:
include::./op/sgdcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=config]

We leave the learning rates at the defaults recommended by Keras.

When we see the accuracy of the prediction with test data and the tie taken for training we get the following plot:

include::./op/Nadamcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=special]

We observe that there's an improvement in accuracy. Curious about improvements in accuracy and such high time taken,
we checked the default learning rates of the optimizers recommended by keras, we saw:
[source, json]
----
{
    "sgd": 0.01,
    "adagrad": 0.01,
    "adadelta": 1.0,
    "adam": 0.001,
    "adamax": 0.002,
    "Nadam": 0.002,
    "RMSProp": 0.001,
}
----

Though we see that all other optimizers performed better, except adagrad and adadelta, all other had learning rate
significantly lower than sgd(though sgd performs the fastest).

When we compare the plots:

* adagrad
+
include::./op/Adagradcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* adadelta
+
include::./op/Adadeltacategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* adam
+
include::./op/Adamcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* adamax
+
include::./op/Adamaxcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* Nadam
+
include::./op/Nadamcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

* RMSProp
+
include::./op/RMSpropcategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

In all cases, we see that the training accuracy reaches 100%, it could be an indicator of over-fitting the data.
Hence we checked roc and precision recall curves.
Here, we present the curves of adadelta but all the curves of other optimizers were very much identical:

* ROC curve:
+
include::./op/Adadeltacategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

* Precision recall curve:
+
include::./op/Adadeltacategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

If we check the incorrectly classified outputs:

include::./op/Adadeltacategorical_crossentropyrelul0.01e32b64mDense512reluDense512reluDense10softmax/list.adoc[tags=incorrect]

We see that model has predicted all the values well and the ones that it missed are in the cases where inputs were
not clear or the shape is very unconventional almost to the point that even a person could mis-recognize at first glance.

Since, our model reaches 100% way earlier than sgd for all the optimizer that we used, we can reduce the number of
epochs to stop training early and prevent over-fitting.

If we consider the time plots shown previously, we can see that with adamax, adagrad and adadelta reaches 100% in < 10 epochs.
Hence, by choosing one of them we can train our model faster. We expect the training time to come down < 50% of current time.

Now we evaluate the model for 5 epochs with adamax, adagrad and adadelta. The results we obtained were:

include::./op/Adamaxcategorical_crossentropyrelul0.01e5b64mDense512reluDense512reluDense10softmax/list.adoc[tags=special]

Looking at the plots adamax seems to be away from over-fitting(in comparison to others) and fastest as well.
Also, the metrics plot of adamax:

include::./op/Adamaxcategorical_crossentropyrelul0.01e5b64mDense512reluDense512reluDense10softmax/list.adoc[tags=metrics]

Then roc:

include::./op/Adamaxcategorical_crossentropyrelul0.01e5b64mDense512reluDense512reluDense10softmax/list.adoc[tags=roc_curve]

Precision recall:

include::./op/Adamaxcategorical_crossentropyrelul0.01e5b64mDense512reluDense512reluDense10softmax/list.adoc[tags=precision_recall_curve]

If we check the confusion matrix:

include::./op/Adamaxcategorical_crossentropyrelul0.01e5b64mDense512reluDense512reluDense10softmax/list.adoc[tags=cmatrix]

Other than classification of label 9, everything else seems to be well.

include::./op/Adamaxcategorical_crossentropyrelul0.01e5b64mDense512reluDense512reluDense10softmax/list.adoc[tags=incorrect]
Now if we see our model has performed best possible at very less training time.
The error are very 'forgivable' especially the 5s, 6, 3 and 2 are distorted and our model though wrong in these examples
has approximated the features to the best available ones.

=== Conclusion
After running some more experiments for lower number of neurons and layers under the last configuration:
include::./op/Adamaxcategorical_crossentropyrelul0.01e5b64mDense512reluDense512reluDense10softmax/list.adoc[tags=config]

we observe that we achieved almost 'similar' levels of accuracy even at 1 hidden layer and 128 neurons.
To make sure that we are not missing something important, we did even shuffle the data and we were able to see similar
results and behavior.

Whether our experimentation has led us to a sweet spot of the hyper-parameters and hence we can achieve the same result
with lesser layers or whether our model from last section:

include::./op/Adamaxcategorical_crossentropyrelul0.01e5b64mDense512reluDense512reluDense10softmax/list.adoc[tags=model]

has too many weights much of which has almost no impact on the outcome is a question needs to be explored further.

